{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Tokenization Demo\n",
    "\n",
    "This notebook shows you how to:\n",
    "1. Install **NLTK** (Natural Language Toolkit) in Google Colab.\n",
    "2. Download necessary data (like the `punkt` tokenizer model).\n",
    "3. Perform sentence tokenization using **`sent_tokenize()`**.\n",
    "4. Perform word tokenization using **`word_tokenize()`** and **WordPunctTokenizer**.\n",
    "5. (Optional) Explore the **RegexpTokenizer** for custom tokenization.\n",
    "\n",
    "By the end, you’ll understand how to quickly split text into sentences or words using NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Import Libraries\n",
    "\n",
    "We’ll install `nltk` if needed and import it. Then we’ll download some data packages, notably `punkt` for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "name": "Install and Download Data",
     "schema_version": 1,
     "provenance": []
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --quiet nltk\n",
    "import nltk\n",
    "nltk.download('punkt')  # This downloads the tokenizer models needed for sent_tokenize and word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Text\n",
    "We’ll define a short multiline text to test out our tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "name": "Define Sample Text",
     "schema_version": 1,
     "provenance": []
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "NLTK is a leading platform for building Python programs to work with human language data.\n",
    "It provides easy-to-use interfaces to over 50 corpora and lexical resources.\n",
    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentence Tokenization\n",
    "\n",
    "NLTK’s `sent_tokenize()` function uses the **Punkt Sentence Tokenizer**. This model learns sentence boundaries based on punctuation and capitalization. It’s effective for many well-punctuated texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "name": "Sentence Tokenization",
     "schema_version": 1,
     "provenance": []
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(sample_text)\n",
    "print(\"=== Sentence Tokenization ===\\n\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Tokenization\n",
    "Here, we’ll look at two popular approaches in NLTK:\n",
    "1. **`word_tokenize()`** – A general-purpose word tokenizer that splits on whitespace and separates punctuation.\n",
    "2. **WordPunctTokenizer** – Splits all punctuation into separate tokens.\n",
    "   - For example, \"I'm\" would be split into `[\"I\", \"'\", \"m\"]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "name": "Word Tokenize",
     "schema_version": 1,
     "provenance": []
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, WordPunctTokenizer\n",
    "\n",
    "# Using word_tokenize()\n",
    "words_default = word_tokenize(sample_text)\n",
    "print(\"=== Word Tokenizer (word_tokenize) ===\\n\")\n",
    "print(words_default)\n",
    "\n",
    "# Using WordPunctTokenizer\n",
    "word_punct = WordPunctTokenizer()\n",
    "words_punct = word_punct.tokenize(sample_text)\n",
    "print(\"\\n=== WordPunctTokenizer ===\\n\")\n",
    "print(words_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optional: RegexpTokenizer\n",
    "If you need **custom** tokenization rules (like ignoring all punctuation, or splitting tokens in a specific way), you can use **RegexpTokenizer** with a custom regular expression. For example, the expression `r'\\w+'` keeps only alphanumeric characters and underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "name": "RegexpTokenizer Example",
     "schema_version": 1,
     "provenance": []
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "regex_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words_regex = regex_tokenizer.tokenize(sample_text)\n",
    "print(\"=== RegexpTokenizer (\\w+) ===\\n\")\n",
    "print(words_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "- **`sent_tokenize()`** splits text into sentences using NLTK’s Punkt model.\n",
    "- **`word_tokenize()`** splits sentences into words and punctuation tokens.\n",
    "- **WordPunctTokenizer** divides punctuation into separate tokens, which can be helpful if you want punctuation to be treated as distinct items.\n",
    "- **RegexpTokenizer** is highly customizable for domain-specific tokenization.\n",
    "\n",
    "#### Tips:\n",
    "- Always download the relevant NLTK data (e.g., `punkt`) when first running a notebook.\n",
    "- For large-scale or more advanced NLP tasks, you might explore other tokenizers (e.g., Byte Pair Encoding, WordPiece) in libraries like [Hugging Face Tokenizers](https://github.com/huggingface/tokenizers).\n",
    "\n",
    "### Next Steps\n",
    "- Perform additional text processing (stopword removal, stemming/lemmatization).\n",
    "- Try analyzing or cleaning real-world text data (tweets, news articles, etc.)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "nltk_tokenization_demo.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
